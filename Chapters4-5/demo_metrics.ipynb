{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mti\n",
    "import nab\n",
    "import rbo\n",
    "import auto_param\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from Chap4_other_metrics import etapr, vus, affiliation, pate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of computing all metrics for an example dataset\n",
    "\n",
    "def get_thresholds_fpr_pos(labels, score, thresholds, max_fpr=0.1):\n",
    "    for i_thresh, thresh in enumerate(thresholds):\n",
    "        pred = np.zeros(score.size)\n",
    "        pred[score > thresh] = 1\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(labels, pred).ravel()\n",
    "        if tn:\n",
    "            fpr = fp / (fp+tn)\n",
    "            if fpr <= max_fpr:\n",
    "                return i_thresh\n",
    "    return len(thresholds)-1\n",
    "\n",
    "def get_thresholds_fpr(labels, score, thresholds, max_fpr=0.1):\n",
    "    thresholds_fpr = list()\n",
    "    for thresh in thresholds:\n",
    "        pred = np.zeros(score.size)\n",
    "        pred[score > thresh] = 1\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(labels, pred).ravel()\n",
    "        if tn:\n",
    "            fpr = fp / (fp+tn)\n",
    "            if fpr <= max_fpr:\n",
    "                thresholds_fpr.append(thresh)\n",
    "    return np.array(thresholds_fpr)\n",
    "\n",
    "def compute_all_metrics(metrics: list, metrics_names, tresholded_metrics: list, labels, scores, scores_names, n_thresholds=100, pos_label=1, max_fpr=0.1):\n",
    "    metrics_outputs = np.zeros((scores.shape[1], len(metrics)))\n",
    "    thresholds = dict()\n",
    "    for i_score in range(scores.shape[1]):\n",
    "        score = scores[:, i_score]\n",
    "        thresholds_raw = np.linspace(np.quantile(score, 0.01), np.quantile(score, 0.99), n_thresholds)\n",
    "        thresholds[i_score] = get_thresholds_fpr(labels, score, thresholds_raw, max_fpr)\n",
    "        for j_metric, metric in enumerate(metrics):\n",
    "            start_time = time.time()\n",
    "            print(metrics_names[j_metric])\n",
    "            if tresholded_metrics[j_metric]:\n",
    "                thresh_values = np.zeros(n_thresholds)\n",
    "                for j_thresh, thresh in enumerate(thresholds[i_score]):\n",
    "                    ij_pred = np.zeros(score.size)\n",
    "                    ij_pred[score > thresh] = 1\n",
    "                    try:\n",
    "                        thresh_values[j_thresh] = metric(labels, ij_pred, pos_label=1)\n",
    "                    except ValueError:\n",
    "                        thresh_values[j_thresh] = 0\n",
    "                metrics_outputs[i_score, j_metric] = thresh_values[np.argmax(np.abs(thresh_values))]\n",
    "            else:\n",
    "                metrics_outputs[i_score, j_metric] = metric(labels, score, max_fpr=max_fpr)\n",
    "\n",
    "    df_metrics = pd.DataFrame(metrics_outputs, columns=metrics_names, index=scores_names)\n",
    "    return df_metrics\n",
    "\n",
    "\n",
    "def compute_all_metrics_rbo(ground_truth_ranking: list, rbo_params: list, metrics: list, metrics_names, tresholded_metrics: list, \n",
    "                            labels, scores, scores_names, n_thresholds=100, pos_label=1, max_fpr=0.1):\n",
    "    df_metrics = compute_all_metrics(metrics, metrics_names, tresholded_metrics, labels, scores, scores_names, n_thresholds, pos_label, max_fpr)\n",
    "    rbo_values = np.zeros((len(rbo_params), len(metrics_names)))\n",
    "    for i_metric in range(len(metrics_names)):\n",
    "        df_metrics = df_metrics.sort_values(by=metrics_names[i_metric], ascending=False)\n",
    "        i_ranking = df_metrics.index.to_numpy()\n",
    "        print(metrics_names[i_metric], i_ranking)\n",
    "        for j_param, param in enumerate(rbo_params):\n",
    "            rbo_values[j_param, i_metric] = rbo.rbo(ground_truth_ranking, i_ranking, p=param)\n",
    "\n",
    "    df_rbo = pd.DataFrame(rbo_values, columns=metrics_names, index=rbo_params)\n",
    "    return df_rbo\n",
    "\n",
    "def mti_compute(labels, pred, pos_label):\n",
    "    return  mti.mti(labels, pred, pos_label=pos_label)\n",
    "\n",
    "def nab_compute(labels, pred, pos_label):\n",
    "    return nab.nab(labels, pred, pos_label=pos_label)\n",
    "\n",
    "def etapr_compute(labels, pred, pos_label):\n",
    "    return etapr.evaluate_w_streams(labels, pred)['f1']\n",
    "\n",
    "def pate_compute(labels, score, max_fpr):\n",
    "    return pate.PATE(labels, score, pos_label=1, num_desired_thresholds=100)\n",
    "\n",
    "def mcc_compute(labels, pred, pos_label):\n",
    "    return metrics.matthews_corrcoef(labels, pred)\n",
    "\n",
    "def VUS_compute(labels, score, max_fpr):\n",
    "    return vus.generate_curve(y_true=labels, y_score=score, slidingWindow=50)[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('Chap4_SyntheticExamples/synthetic_simple_cases.csv')\n",
    "\n",
    "# df0.iloc[2500:2850, 1] = df0.iloc[2150:2500, 1]\n",
    "\n",
    "labels = df0.labels.values\n",
    "labels[labels == 1] = 0\n",
    "labels[labels == -1] = 1\n",
    "\n",
    "scores = df0.values[:, 1:]\n",
    "scores_names = df0.columns.to_numpy()[1:]\n",
    "\n",
    "metrics_to_compute = [mti_compute,\n",
    "                       nab_compute, \n",
    "                      metrics.f1_score, metrics.roc_auc_score,\n",
    "                       affiliation.affiliation_f1_score, \n",
    "                      etapr_compute, mcc_compute, VUS_compute, pate_compute\n",
    "                      ]\n",
    "metrics_name = ['mti', \n",
    "                'NAB', 'F1', 'AUC',\n",
    "                 'Affiliation',\n",
    "                 'eTaPR', \n",
    "                 'MCC',\n",
    "                   'VUS', 'PATE'\n",
    "                 ]\n",
    "metrics_thresholded = [True, \n",
    "                       True, True, False, \n",
    "                       True, \n",
    "                       True, True, False, False\n",
    "                       ]\n",
    "\n",
    "ground_truth = ['A', 'B', 'C', 'D', 'E']\n",
    "rbo_params = [0.2, 0.5, 1]\n",
    "\n",
    "print(compute_all_metrics_rbo(ground_truth, rbo_params, metrics_to_compute, metrics_name, metrics_thresholded, labels, scores, scores_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AutoParam example\n",
    "\n",
    "df_ex2 = pd.read_csv('Chap5_SyntheticCollections/df_onearea_auto.csv')\n",
    "\n",
    "def mti_get_simple_components(y_true, y_pred, pos_label=1):\n",
    "    mti_metric = mti.MTI(anticipation_period=\"default\", earliness_period=\"default\", inertia_delay=\"default\")\n",
    "    mti_metric.compute_metrics(y_true, y_pred, pos_label)\n",
    "    return [mti_metric.recall_score, mti_metric.masked_specificity_score, mti_metric.anticipation_score, mti_metric.alarm_cardinality_score]\n",
    "\n",
    "labels = df_ex2.labels.values\n",
    "gt_ranking = [['Full'], ['Two First Half'], ['Two Spaced'], ['Inertia_detect'], ['Detect + FP'], ['Reverse']]\n",
    "names = ['Full', 'Two First Half', 'Two Spaced', 'Inertia_detect', 'Detect + FP', 'Reverse']\n",
    "\n",
    "mti_components = np.zeros((len(gt_ranking), 4))\n",
    "for i_d, dataset in enumerate(gt_ranking):\n",
    "    tmp_pred = df_ex2[dataset[0]].values\n",
    "    mti_components[i_d, :] = mti_get_simple_components(labels, tmp_pred)\n",
    "\n",
    "predictions = [df_ex2['Full'].values, df_ex2['Two First Half'].values, df_ex2['Two Spaced'].values, df_ex2['Inertia_detect'].values, \n",
    "               df_ex2['Detect + FP'].values, df_ex2['Reverse'].values]\n",
    "\n",
    "params_mti = {'component': mti_components, 'anticip_areas': [[390, 400]], 'anticip_early_len': [30]}\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "coef_start = np.random.uniform(low=-20, high=20, size=100)\n",
    "Wr_start = np.random.uniform(low=0.01, high=100, size=100)\n",
    "Wspem_start = np.random.uniform(low=0.01, high=100, size=100)\n",
    "Wcardal_start = np.random.uniform(low=0.01, high=100, size=100)\n",
    "\n",
    "sols_mti_ex2 = np.zeros((100, 4))\n",
    "rbo_mti_ex2 = np.zeros(100)\n",
    "time_torczon_mti_ex2 = np.zeros(100)\n",
    "for i in range(100):\n",
    "    t_start = time.time()\n",
    "    auto_mti = auto_param.AutoProfile(x0=[coef_start[i], Wr_start[i], Wspem_start[i], Wcardal_start[i]], \n",
    "                           xmin=[-20, 0, 0, 0], xmax=[20, 100, 100, 100], Nguess=20, Niter=50, rbo_p=[0.2, 1])\n",
    "    auto_mti.fit(gt_ranking, names, predictions, params=params_mti, metric='MTI')\n",
    "    time_torczon_mti_ex2[i] = time.time() - t_start\n",
    "    rbo_mti_ex2[i] = auto_mti.solution.f\n",
    "    sols_mti_ex2[i, :] = auto_mti.solution.x\n",
    "    if not i % 20:\n",
    "        print(i, np.mean(rbo_mti_ex2[:i+1]))\n",
    "\n",
    "df_ex2_mti = pd.DataFrame()\n",
    "df_ex2_mti['rbo'] = rbo_mti_ex2\n",
    "df_ex2_mti['time'] = time_torczon_mti_ex2\n",
    "df_ex2_mti[['coefs', 'Wrs', 'Wspems', 'Wcardals']] = sols_mti_ex2\n",
    "# df_ex1_mti.to_csv('Results_AutoParam/ex2_autoparam_mti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nab = {'label': labels}\n",
    "np.random.seed(0)\n",
    "\n",
    "coef_start = np.random.uniform(low=-20, high=-0.01, size=100)\n",
    "Afp_start = np.random.uniform(low=0.01, high=100, size=100)\n",
    "Afn_start = np.random.uniform(low=0.01, high=100, size=100)\n",
    "sols_nab_ex2 = np.zeros((100, 3))\n",
    "rbo_nab_ex2 = np.zeros(100)\n",
    "time_torczon_nab_ex2 = np.zeros(100)\n",
    "\n",
    "for i in range(100):\n",
    "    t_start = time.time()\n",
    "    auto_nab = auto_param.AutoProfile(x0=[coef_start[i], Afp_start[i], Afn_start[i]], xmin=[-20, 0.01, 0.01], xmax=[-0.01, 100, 100], Nguess=20, Niter=50, rbo_p=[0.2, 1])\n",
    "    auto_nab.fit(gt_ranking, names, predictions, params=params_nab, metric='NAB')\n",
    "    time_torczon_nab_ex2[i] = time.time() - t_start\n",
    "    rbo_nab_ex2[i] = auto_nab.solution.f\n",
    "    sols_nab_ex2[i, :] = auto_nab.solution.x\n",
    "    if not i % 5:\n",
    "        print(i, np.mean(rbo_nab_ex2[:i+1]))\n",
    "\n",
    "df_ex2_nab = pd.DataFrame()\n",
    "df_ex2_nab['rbo'] = rbo_nab_ex2\n",
    "df_ex2_nab['time'] = time_torczon_nab_ex2\n",
    "df_ex2_nab[['coefs', 'Afps', 'Afns']] = sols_nab_ex2\n",
    "# df_ex2_nab.to_csv('ex2_autoparam_nab.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
